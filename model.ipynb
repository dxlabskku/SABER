{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from utils import create_resample, normalize\n",
    "import numpy as np \n",
    "from numpy import asarray,zeros, array\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from keras import backend as k\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from keras.layers import LSTM,Dropout,Bidirectional,Input,Embedding, Dense,Concatenate,Flatten, Multiply,Average,Reshape,Lambda\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from keras.utils import to_categorical\n",
    "from keras import optimizers,regularizers\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import random,statistics,json\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.utils import resample\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attentionScores(var):\n",
    "    Q_t, K_t, V_t = var[0],var[1],var[2]\n",
    "    scores = tf.matmul(Q_t, K_t, transpose_b=True)\n",
    "    distribution = tf.nn.softmax(scores)\n",
    "    scores = tf.matmul(distribution, V_t)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/climate_change_tweets.csv\")\n",
    "features = normalize(data[['tweet_id', 'prepContent', 'len_text', 'num_hashtag', 'num_emoji', 'num_mention', \n",
    "                           'degC', 'clsC', 'btwC', 'engage', 'pagerank']])\n",
    "labels = data[['stance_label', 'vader_label']]\n",
    "\n",
    "prep_id = features.tweet_id.values[i]\n",
    "prep_text = features.prepContent.values[i]\n",
    "prep_senti = labels.vader_label.values[i]\n",
    "prep_stance = labels.stance_label.values[i]\n",
    "prep_graph = data[['len_text', 'num_hashtag', 'num_emoji', 'num_mention', \n",
    "                          'degC', 'clsC', 'btwC', 'engage', 'pagerank']].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### converting stance labels into categorical labels ########\n",
    "label_encoder = LabelEncoder()\n",
    "final_lbls = prep_stance\n",
    "values = array(final_lbls)\n",
    "total_integer_encoded = label_encoder.fit_transform(values)\n",
    "\n",
    "prep_enc_stance = total_integer_encoded\n",
    "total_integer_encoded = to_categorical(total_integer_encoded)\n",
    "\n",
    "prep_stance = total_integer_encoded\n",
    "\n",
    "########### converting senti labels into categorical labels ########\n",
    "label_encoder = LabelEncoder()\n",
    "final_lbls = prep_senti\n",
    "values = array(final_lbls)\n",
    "total_integer_encoded=label_encoder.fit_transform(values)\n",
    "\n",
    "prep_enc_senti = total_integer_encoded\n",
    "total_integer_encoded = to_categorical(total_integer_encoded)\n",
    "\n",
    "prep_senti = total_integer_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ = 150\n",
    "MAX_LENGTH = 150\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(total_text)\n",
    "total_sequence = tokenizer.texts_to_sequences(total_text)\n",
    "padded_docs = pad_sequences(total_sequence, maxlen=MAX_SEQ, padding='post')\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "print(\"downloading {} embedding ...\".format(EMB_TYPE))\n",
    "embedding_matrix = np.load(\"./dataset/embed_matrix_glove.npy\")\n",
    "\n",
    "print(\"embedding matrix ****************\",embedding_matrix.shape)\n",
    "print(\"non zeros bert :\", sum(np.all(embedding_matrix, axis=1)))\n",
    "\n",
    "total_emb = np.load(\"./dataset/embed_node_gcn_20240121.npy\")\n",
    "total_graph = prep_graph\n",
    "total_feat_sequence = np.concatenate((padded_docs, total_graph), axis=1)\n",
    "total_feat_sequence = np.concatenate((total_feat_sequence, total_emb), axis=1)\n",
    "\n",
    "emb_len = embedding_matrix.shape[1]\n",
    "emb_feat_len = total_feat_sequence.shape[1]\n",
    "print(\"input matrix ****************\", total_feat_sequence.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_labels_stance = np.array(prep_stance)\n",
    "total_labels_senti = np.array(prep_senti)\n",
    "total_sequence = np.array(padded_docs)\n",
    "\n",
    "total_labels_stance_enc = np.argmax(total_labels_stance, axis=1)\n",
    "list_acc_stance, list_acc_senti = [], []\n",
    "list_f1_stance, list_f1_senti = [], []\n",
    "\n",
    "kf = StratifiedKFold(n_splits=5, random_state=None,shuffle=False)\n",
    "fold = 0\n",
    "results = []\n",
    "for train_index, test_index in kf.split(total_sequence, total_labels_stance_enc):\n",
    "    print(\"K FOLD ::::::\", fold)\n",
    "    fold = fold + 1\n",
    "\n",
    "    ############## Stance inputs #############\n",
    "    FEAT_LENGTH = 9\n",
    "    NODEEMB_LENGTH = 32\n",
    "    \n",
    "    W = tf.Variable(1.0 ,trainable=True)\n",
    "    input1_stance = Input (shape = (MAX_LENGTH,)) # MAX_LENGTH = 150\n",
    "    input2_stance = Input (shape=(FEAT_LENGTH,)) # FEAT_LENGTH = 9\n",
    "    input3_stance = Input (shape=(NODEEMB_LENGTH,1)) # NODEEMB_LENGTH = 32\n",
    "    \n",
    "    input1_text_stance = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=150, name='text_embed_stance')(input1_stance) \n",
    "    input2_text_stance = Dense(50)(input2_stance) \n",
    "    \n",
    "    lstm_stance = Bidirectional(LSTM(100, name='lstm_inp1_stance',activation='tanh',dropout=.2,kernel_regularizer=regularizers.l2(0.07)))(input1_text_stance)\n",
    "    lstm_nodeemb = Bidirectional(LSTM(100, name='lstm_inp3_nodeemb',activation='tanh',dropout=.2,kernel_regularizer=regularizers.l2(0.07)))(input3_stance)\n",
    "    text_final_stance = Dense(100, activation=\"relu\")(lstm_stance)\n",
    "    text_emb_stance = Dense(100, activation=\"relu\")(lstm_nodeemb)\n",
    "    text_final_stance = Concatenate()([text_final_stance, input2_text_stance, W*text_emb_stance])\n",
    "\n",
    "    Q_t = Dense(100, activation=\"relu\")(text_final_stance)\n",
    "    K_t = Dense(100, activation=\"relu\")(text_final_stance)\n",
    "    V_t = Dense(100, activation=\"relu\")(text_final_stance)\n",
    "    IA_text_stance = Lambda(attentionScores)([Q_t,K_t,V_t])\n",
    "    T1 = IA_text_stance\n",
    "    \n",
    "    ### capturing weights ######\n",
    "    try_1 = layers.Dense(100, activation=\"relu\")\n",
    "    T1_try = try_1(T1)\n",
    "    try_1_weights = try_1.get_weights()\n",
    "\n",
    "    stance_specific_output = Dense(2, activation=\"softmax\", name=\"task_specific_act\")(T1_try)\n",
    "\n",
    "    ############## Sentiment inputs #############\n",
    "    W2 = tf.Variable(1.0 ,trainable=True)\n",
    "    input1_senti = Input (shape = (MAX_LENGTH,))\n",
    "    input2_senti = Input (shape=(FEAT_LENGTH,))\n",
    "    input3_senti = Input (shape=(NODEEMB_LENGTH,1))\n",
    "    \n",
    "    input1_text_senti = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=150, name='text_embed_senti')(input1_senti) \n",
    "    input2_text_senti = Dense(50)(input2_senti) \n",
    "\n",
    "    lstm_senti = Bidirectional(LSTM(100, name='lstm_inp1_senti',activation='tanh',dropout=.2,kernel_regularizer=regularizers.l2(0.07)))(input1_text_senti)\n",
    "    lstm_senti_nodeemb = Bidirectional(LSTM(100, name='lstm_inp3_sent_nodeemb',activation='tanh',dropout=.2,kernel_regularizer=regularizers.l2(0.07)))(input3_senti)\n",
    "    text_final_senti = Dense(100, activation=\"relu\")(lstm_senti)\n",
    "    text_emb_senti = Dense(100, activation=\"relu\")(lstm_senti_nodeemb)\n",
    "    text_final_senti = Concatenate()([text_final_senti, input2_text_senti, W2*text_emb_senti])\n",
    "    \n",
    "    Q_t = Dense(100, activation=\"relu\")(text_final_senti)\n",
    "    K_t = Dense(100, activation=\"relu\")(text_final_senti)\n",
    "    V_t = Dense(100, activation=\"relu\")(text_final_senti)\n",
    "    IA_text_senti = Lambda(attentionScores)([Q_t,K_t,V_t])\n",
    "    T2 = IA_text_senti\n",
    "    try_2 = layers.Dense(100, activation=\"relu\")\n",
    "    T2_try = try_2(T2)\n",
    "    try_2_weights = try_2.get_weights()\n",
    "\n",
    "    senti_specific_output = Dense(3, activation=\"softmax\", name=\"task_specific_senti\")(T2_try)\n",
    "\n",
    "    ################ Average of 3 tensors ##############\n",
    "    M=Average()([T1_try,T2_try])\n",
    "    print(\"M::::\",M)\n",
    "\n",
    "    ############# shared specific attention for stance ###############\n",
    "    G_stance_layer = layers.Dense(100, activation=\"sigmoid\")\n",
    "    G_stance = G_stance_layer(M)\n",
    "    G_stance_layer.set_weights(try_1_weights)\n",
    "    G_stance_out = Multiply()([G_stance,M])\n",
    "\n",
    "    Shared_query = Dense(100, activation=\"relu\")(T1_try)\n",
    "    Shared_key = Dense(100, activation=\"relu\")(M)\n",
    "    Shared_value = Dense(100, activation=\"relu\")(M)\n",
    "    Shared_stance_attn = Lambda(attentionScores)([Shared_query,Shared_key,Shared_value])\n",
    "\n",
    "    Diff = layers.subtract([G_stance_out,Shared_stance_attn])\n",
    "    fuse_mul = Multiply()([G_stance_out,Shared_stance_attn])\n",
    "    fuse_stance = Concatenate()([G_stance_out,Shared_stance_attn,Diff,fuse_mul])\n",
    "    fuse_stance_shared = Dense(100, activation=\"tanh\")(fuse_stance)\n",
    "\n",
    "    ############# shared specific attention for sentiment ###############\n",
    "    G_sent_layer = layers.Dense(100, activation=\"sigmoid\")\n",
    "    G_sent = G_sent_layer(M)\n",
    "    G_sent_layer.set_weights(try_2_weights)\n",
    "    G_sent_out=Multiply()([G_sent,M])\n",
    "\n",
    "    Shared_query = Dense(100, activation=\"relu\")(T2_try)\n",
    "    Shared_key = Dense(100, activation=\"relu\")(M)\n",
    "    Shared_value = Dense(100, activation=\"relu\")(M)\n",
    "    Shared_sent_attn = Lambda(attentionScores)([Shared_query,Shared_key,Shared_value])\n",
    "\n",
    "    Diff = layers.subtract([G_sent_out, Shared_sent_attn])\n",
    "    fuse_mul = Multiply()([G_sent_out, Shared_sent_attn])\n",
    "    fuse_sent = Concatenate()([G_sent_out, Shared_sent_attn, Diff, fuse_mul])\n",
    "    fuse_sent_shared = Dense(100, activation=\"tanh\")(fuse_sent)\n",
    "\n",
    "    stance_shared_output = Dense(2, activation=\"softmax\", name=\"task_stance_shared\")(fuse_stance_shared)\n",
    "    senti_shared_output = Dense(3, activation=\"softmax\", name=\"task_senti_shared\")(fuse_sent_shared)\n",
    "\n",
    "    model = Model([input1_stance,input2_stance,input3_stance,input1_senti,input2_senti,input3_senti],\n",
    "                  [stance_specific_output,senti_specific_output,stance_shared_output,senti_shared_output])\n",
    "\n",
    "    ############ K fold data ############\n",
    "    test_feat_sequence, train_feat_sequence = total_feat_sequence[test_index], total_feat_sequence[train_index]\n",
    "    test_sequence = test_feat_sequence[:, :MAX_LENGTH] \n",
    "    test_onlyfeat_sequence = test_feat_sequence[:, MAX_LENGTH:MAX_LENGTH+FEAT_LENGTH]\n",
    "    test_emb_sequence = test_feat_sequence[:, -NODEEMB_LENGTH:]\n",
    "    \n",
    "    test_feat_sequence = tf.convert_to_tensor(test_feat_sequence.astype(np.float32))\n",
    "    test_sequence = tf.convert_to_tensor(test_sequence.astype(np.float32))\n",
    "    test_onlyfeat_sequence = tf.convert_to_tensor(test_onlyfeat_sequence.astype(np.float32))\n",
    "    test_emb_sequence = tf.convert_to_tensor(test_emb_sequence.astype(np.float32))\n",
    "    \n",
    "    test_stance, train_stance = total_labels_stance[test_index], total_labels_stance[train_index]\n",
    "    test_senti,train_senti = total_labels_senti[test_index], total_labels_senti[train_index]\n",
    "    test_enc = np.argmax(test_stance, axis=1)\n",
    "    train_enc = np.argmax(train_stance, axis=1)\n",
    "    train_enc_senti = np.argmax(train_senti, axis=1)\n",
    "    print(\"len of train\", np.unique(train_enc,return_counts=True),len(train_stance))\n",
    "    print(\"len of test\", np.unique(test_enc,return_counts=True),len(test_stance))\n",
    "    \n",
    "    # oversample\n",
    "    train_feat_sequence, train_enc, train_enc_senti = create_resample(train_feat_sequence, train_enc, train_enc_senti)\n",
    "    train_feat_sequence = tf.convert_to_tensor(train_feat_sequence)\n",
    "    train_sequence = train_feat_sequence[:, :MAX_LENGTH]\n",
    "    train_sequence = tf.convert_to_tensor(train_sequence)\n",
    "    train_onlyfeat_sequence = train_feat_sequence[:,  MAX_LENGTH:MAX_LENGTH+FEAT_LENGTH]\n",
    "    train_onlyfeat_sequence = tf.convert_to_tensor(train_onlyfeat_sequence)\n",
    "    train_emb_sequence = train_feat_sequence[:, -NODEEMB_LENGTH:]\n",
    "    train_emb_sequence = tf.convert_to_tensor(train_emb_sequence)\n",
    "    \n",
    "    train_stance = to_categorical(train_enc)\n",
    "    train_senti = to_categorical(train_enc_senti)\n",
    "\n",
    "    model.compile(optimizer=Adam(0.0001),loss={'task_specific_act':'binary_crossentropy','task_specific_senti':'categorical_crossentropy','task_stance_shared':'binary_crossentropy','task_senti_shared':'categorical_crossentropy'}, loss_weights={'task_specific_act':1.0,'task_specific_senti':0.5,'task_senti_shared':0.5,'task_stance_shared':1.0}, metrics=['accuracy'])    \n",
    "    print(model.summary())\n",
    "\n",
    "    model.fit([train_sequence, train_onlyfeat_sequence, train_emb_sequence, train_sequence, train_onlyfeat_sequence, train_emb_sequence],\n",
    "              [train_stance, train_senti, train_stance, train_senti], shuffle=True,validation_split=0.2,epochs=30,verbose=2)\n",
    "    predicted = model.predict([test_sequence, test_onlyfeat_sequence, test_emb_sequence, \n",
    "                               test_sequence, test_onlyfeat_sequence, test_emb_sequence])\n",
    "\n",
    "    test_enc = np.argmax(test_stance, axis=1)\n",
    "    stance_pred_specific = predicted[0]\n",
    "    stance_pred_shared = predicted[2]\n",
    "    result_ = np.mean([stance_pred_specific,stance_pred_shared],axis=0)\n",
    "    p_1 = np.argmax(result_, axis=1)\n",
    "    test_accuracy = accuracy_score(test_enc, p_1)\n",
    "    list_acc_stance.append(test_accuracy)\n",
    "    print(\"test accuracy::::\",test_accuracy)\n",
    "    target_names = ['believe','deny']\n",
    "    class_rep=classification_report(test_enc, p_1)\n",
    "    print(\"specific confusion matrix\", confusion_matrix(test_enc, p_1))\n",
    "    print(class_rep)\n",
    "    class_rep = classification_report(test_enc, p_1, target_names=target_names,output_dict=True)\n",
    "    macro_avg = class_rep['macro avg']['f1-score']\n",
    "    print(\"macro f1 score\",macro_avg)\n",
    "    list_f1_stance.append(macro_avg)\n",
    "\n",
    "    ########### sentiment\n",
    "    test_enc_senti = np.argmax(test_senti, axis=1)\n",
    "\n",
    "    sent_pred_specific = predicted[1]\n",
    "    sent_pred_shared = predicted[3]\n",
    "    result_ = np.mean([sent_pred_specific,sent_pred_shared],axis=0)\n",
    "    p_1 = np.argmax(result_, axis=1)\n",
    "    test_accuracy = accuracy_score(test_enc_senti, p_1)\n",
    "    list_acc_senti.append(test_accuracy)\n",
    "    print(\"test accuracy::::\",test_accuracy)\n",
    "    target_names = ['neg', 'neu', 'pos']\n",
    "    class_rep = classification_report(test_enc_senti, p_1)\n",
    "    print(\"specific confusion matrix\", confusion_matrix(test_enc_senti, p_1))\n",
    "    print(class_rep)\n",
    "    class_rep = classification_report(test_enc_senti, p_1, target_names=target_names, output_dict=True)\n",
    "    macro_avg = class_rep['macro avg']['f1-score']\n",
    "    print(\"macro f1 score\", macro_avg)\n",
    "    list_f1_senti.append(macro_avg)\n",
    "\n",
    "print(\"ACCURACY :::::::::::: #############\")\n",
    "print(\"act  ::: \", list_acc_stance)\n",
    "print(\"Mean, STD DEV\", statistics.mean(list_acc_stance), statistics.stdev(list_acc_stance))\n",
    "\n",
    "print(\"F1  $$$$$$$$$$$$$$$$$ ::::::::::::\")\n",
    "print(\"Specific F1 ::: \", list_f1_stance)\n",
    "print(\"MTL Mean, STD DEV\", statistics.mean(list_f1_stance), statistics.stdev(list_f1_stance))\n",
    "\n",
    "\n",
    "############# sentiment \n",
    "\n",
    "print(\"ACCURACY :::::::::::: #############\")\n",
    "print(\"act  ::: \", list_acc_senti)\n",
    "print(\"Mean, STD DEV\", statistics.mean(list_acc_senti), statistics.stdev(list_acc_senti))\n",
    "\n",
    "print(\"F1  $$$$$$$$$$$$$$$$$ ::::::::::::\")\n",
    "print(\"Specific F1 ::: \", list_f1_senti)\n",
    "print(\"MTL Mean, STD DEV\", statistics.mean(list_f1_senti), statistics.stdev(list_f1_senti))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mtl_gpu_py39",
   "language": "python",
   "name": "mtl_gpu_py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
